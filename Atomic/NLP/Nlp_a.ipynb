{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo bang!\n",
      "halo juga dek!\n",
      "kenapa nih kalo bolech tahu?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def segmentation(text):\n",
    "\n",
    "    punctuation_marks = [\"?\", \".\", \"!\"]\n",
    "    segments = []\n",
    "\n",
    "    for mark in punctuation_marks:\n",
    "        text = text.replace(mark, mark + \"\\n\")\n",
    "\n",
    "    for segment in text.split(\"\\n\"):\n",
    "        segments.append(segment.strip())\n",
    "    \n",
    "    return \"\\n\".join(segments)\n",
    "\n",
    "    \n",
    "\n",
    "input_text = input()\n",
    "\n",
    "results = segmentation(input_text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would be to\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Function to tokenize text into sentences\n",
    "def tokenize_sentences(text):\n",
    "    sentences = text.split('.')\n",
    "    #print(\"token \",sentences)\n",
    "    result = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return result\n",
    "\n",
    "# Function to tokenize a sentence into words\n",
    "def tokenize_words(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "# Function to find trigrams in a list of words\n",
    "def find_trigrams(words):\n",
    "    trigrams = []\n",
    "    for i in range(len(words) - 2):\n",
    "        trigrams.append(' '.join(words[i:i+3]))\n",
    "        #print(\"trigrams \", trigrams)\n",
    "    return trigrams\n",
    "\n",
    "# Function to find the most frequent trigram\n",
    "def find_most_frequent_trigram(text):\n",
    "    sentences = tokenize_sentences(text)\n",
    "    trigram_counts = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = tokenize_words(sentence)\n",
    "        if len(words) < 3:\n",
    "            continue\n",
    "        \n",
    "        trigrams = find_trigrams(words)\n",
    "        for trigram in trigrams:\n",
    "            trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1\n",
    "    \n",
    "    max_count = 0\n",
    "    most_frequent_trigram = None\n",
    "    for trigram, count in trigram_counts.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            most_frequent_trigram = trigram\n",
    "\n",
    "    #print(\"count \", trigram_counts)\n",
    "    \n",
    "    return most_frequent_trigram\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #s = sys.stdin.read().strip()\n",
    "    s = input()\n",
    "    # Find and print the most frequent trigram\n",
    "    result = find_most_frequent_trigram(s)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I came from the moon\n",
      "['i', 'came', 'from', 'the', 'moon']\n",
      "['i came from', 'came from the', 'from the moon'] \n",
      "\n",
      "He went to the other room\n",
      "['he', 'went', 'to', 'the', 'other', 'room']\n",
      "['he went to', 'went to the', 'to the other', 'the other room'] \n",
      "\n",
      "She went to the drawing room\n",
      "['she', 'went', 'to', 'the', 'drawing', 'room']\n",
      "['she went to', 'went to the', 'to the drawing', 'the drawing room'] \n",
      "\n",
      "{'i came from': 1, 'came from the': 1, 'from the moon': 1, 'he went to': 1, 'went to the': 2, 'to the other': 1, 'the other room': 1, 'she went to': 1, 'to the drawing': 1, 'the drawing room': 1}\n"
     ]
    }
   ],
   "source": [
    "s = \"I came from the moon. He went to the other room. She went to the drawing room.\"\n",
    "\n",
    "x = tokenize_sentences(s)\n",
    "\n",
    "trigrams_counts = {}\n",
    "\n",
    "for i in x:\n",
    "    print(i)\n",
    "    words = tokenize_words(i)\n",
    "    print(words)\n",
    "\n",
    "    if len(words) < 3:\n",
    "        continue\n",
    "\n",
    "    trigrams = find_trigrams(words)\n",
    "    for trigram in trigrams:\n",
    "        trigrams_counts[trigram] = trigrams_counts.get(trigram, 0) + 1\n",
    "    print(trigrams,\"\\n\")\n",
    "\n",
    "print(trigrams_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.41\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Given perplexity\n",
    "perplexity = 170\n",
    "\n",
    "# Compute cross-entropy\n",
    "cross_entropy = math.log2(perplexity)\n",
    "\n",
    "# Round the result to two decimal places\n",
    "cross_entropy_rounded = round(cross_entropy, 2)\n",
    "\n",
    "# Print the result\n",
    "print(cross_entropy_rounded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Url and Hastag Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\n",
      "live in ternet\n",
      "isitme\n"
     ]
    }
   ],
   "source": [
    "def read_dictionary(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return set(word.strip() for word in file)\n",
    "    \n",
    "def segment_input(input_str, dictionary):\n",
    "    input_str = input_str.lower().lstrip('#').replace('www.', '').split('.')[0]  # Ignore 'www.' and extensions\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(input_str):\n",
    "        found = False\n",
    "        for j in range(len(input_str), i, -1):\n",
    "            token = input_str[i:j]\n",
    "            if token in dictionary:\n",
    "                tokens.append(token)\n",
    "                i = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # If no valid token found, append the remaining part of input_str\n",
    "            tokens.append(input_str[i:])\n",
    "            break\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Read the list of words from the file\n",
    "dictionary = read_dictionary(\"dictionary.txt\")\n",
    "\n",
    "# Read the number of test cases\n",
    "num_test_cases = int(input())\n",
    "\n",
    "# Process each test case\n",
    "for _ in range(num_test_cases):\n",
    "    input_str = input()\n",
    "    segmented_input = segment_input(input_str, dictionary)\n",
    "    print(segmented_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackerlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
